import streamlit as st
import pandas as pd
import requests
import time
import io
import zipfile
import re
from typing import List, Dict, Any
from rapidfuzz import fuzz # ğŸ”¹ ë¬¸ìì—´ ìœ ì‚¬ë„ ê³„ì‚°ìš©
import traceback # ìƒì„¸ ì˜¤ë¥˜ ë¡œê¹…ìš©

# Selenium ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ì´ ë²„ì „ì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

# =========================================================
# âš™ï¸ 1. ì„¤ì • ì •ë³´
# =========================================================
KAKAO_REST_API_KEY = "c2c5289d7f9ea5cfa234222f18883527"

TARGET_REGIONS = ["ì„œìš¸", "ê²½ê¸°"]

TARGET_THEMES = [
    "í•œì˜¥ì¹´í˜", "ì˜¤ì…˜ë·° ì¹´í˜", "ë¸ŒëŸ°ì¹˜ ë§›ì§‘", "ë² ì´ì»¤ë¦¬ ì¹´í˜",
    "ìˆ˜ëª©ì›", "ìì—°íœ´ì–‘ë¦¼", "ë¯¸ìˆ ê´€", "ë°•ë¬¼ê´€", "ë¬¸í™” ìœ ì ì§€", "í…Œë§ˆíŒŒí¬",
    # ì¤‘ë³µ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ í‚¤ì›Œë“œ ì¶”ê°€
    "í˜¸ìˆ˜ê³µì›"
]

MAX_API_PAGES = 5
AI_TEST_IMAGE_COUNT = 100
MAX_IMAGES_PER_PLACE = 3  # âœ… ê° ì¥ì†Œë‹¹ ìµœëŒ€ ì´ë¯¸ì§€ ê°œìˆ˜
GPS_SIMILARITY_THRESHOLD = 0.001 # âœ… í•µì‹¬ í‚¤ì›Œë“œ ì¤‘ë³µ ì œê±° ì‹œ ì‚¬ìš©í•  GPS ê·¼ì ‘ ê¸°ì¤€ (ì•½ 100m)

# =========================================================
# 2. API ìš”ì²­ í•¨ìˆ˜
# =========================================================
def search_kakao_local_api(query: str, page: int) -> Dict[str, Any]:
    """Kakao Local API (ì¥ì†Œ ê²€ìƒ‰)"""
    url = "https://dapi.kakao.com/v2/local/search/keyword.json"
    headers = {"Authorization": f"KakaoAK {KAKAO_REST_API_KEY}"}
    params = {"query": query, "size": 15, "page": page}
    try:
        response = requests.get(url, headers=headers, params=params, timeout=10)
        response.raise_for_status()
        return response.json()
    except Exception as e:
        st.error(f"Local API ìš”ì²­ ì˜¤ë¥˜ ({query}, page {page}): {e}")
        return {}


def search_kakao_image_api_multi(query: str, count: int = 3) -> List[str]:
    """Kakao Image Search API (í•œ ì¥ì†Œë‹¹ ì—¬ëŸ¬ ì¥ ì´ë¯¸ì§€)"""
    url = "https://dapi.kakao.com/v2/search/image"
    headers = {"Authorization": f"KakaoAK {KAKAO_REST_API_KEY}"}
    params = {"query": query, "sort": "accuracy", "size": count}
    try:
        response = requests.get(url, headers=headers, params=params, timeout=10)
        response.raise_for_status()
        data = response.json()
        image_urls = [d.get('image_url') for d in data.get('documents', []) if d.get('image_url')]
        processed_urls = []
        for src in image_urls:
            if src and src.startswith('http'):
                if "//t1.daumcdn.net/" in src or "//img1.kakaocdn.net/" in src :
                    src = re.sub(r'\.q\d+', '', src)
                    src = re.sub(r'/[RC]\d+x\d+/', '/origin/', src)
                    src = re.sub(r'[RC]\d+x\d+', 'origin', src)
                processed_urls.append(src)
        return processed_urls[:count]
    except Exception as e:
        st.warning(f"Image API ìš”ì²­ ì˜¤ë¥˜ ({query}): {e}")
        return []


# =========================================================
# âœ¨ 3. ìœ ì‚¬ë„ ë° í•µì‹¬ í‚¤ì›Œë“œ ê¸°ë°˜ ì¤‘ë³µ íŒì • í•¨ìˆ˜
# =========================================================
def get_core_keyword(place_name: str) -> str:
    """ì¥ì†Œëª…ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ(ì•ë¶€ë¶„) ì¶”ì¶œ"""
    parts = place_name.split()
    if len(parts) > 1:
        common_suffixes = ["í’‹ì‚´ì¥", "ì‚°ì±…ê¸¸", "ì…êµ¬", "ì£¼ì°¨ì¥", "ë¬¸í™”ì„¼í„°", "ë„ì„œê´€", "ë¯¸ìˆ ê´€", "ì •ë¬¸", "í›„ë¬¸", "ë§¤í‘œì†Œ", "ë†€ì´í„°"]
        # ë§ˆì§€ë§‰ ë‹¨ì–´ê°€ ì‹œì„¤ëª…ì´ê³ , ê·¸ ì• ë‹¨ì–´ê°€ ìˆë‹¤ë©´ ì•ë¶€ë¶„ì„ í•µì‹¬ìœ¼ë¡œ ê°„ì£¼
        if parts[-1] in common_suffixes and len(parts) > 1 :
             # ì˜ˆ: "ì„œì„œìš¸í˜¸ìˆ˜ê³µì› í’‹ì‚´ì¥" -> "ì„œì„œìš¸í˜¸ìˆ˜ê³µì›"
             # ì˜ˆ: "ì–´ë¦°ì´ëŒ€ê³µì› ì •ë¬¸" -> "ì–´ë¦°ì´ëŒ€ê³µì›"
            return " ".join(parts[:-1])
        # ë˜ëŠ” íŠ¹ì • í‚¤ì›Œë“œë¡œ ì‹œì‘í•˜ë©´ ê·¸ê²ƒì„ í•µì‹¬ìœ¼ë¡œ (ì˜ˆ: ì¹´í˜ ì´ë¦„)
        # elif parts[0] in ["ìŠ¤íƒ€ë²…ìŠ¤", "íˆ¬ì¸í”Œë ˆì´ìŠ¤"]: return parts[0] + " " + parts[1] # "ìŠ¤íƒ€ë²…ìŠ¤ OOì "
    # ê¸°ë³¸ì ìœ¼ë¡œëŠ” ì²« ë‹¨ì–´ ë˜ëŠ” ì „ì²´ ì´ë¦„ ë°˜í™˜ (ì§§ì€ ì´ë¦„ ëŒ€ë¹„)
    return parts[0] if len(parts) > 1 else place_name


def is_gps_close(new_place: Dict[str, Any], existing_place: Dict[str, Any], threshold: float) -> bool:
    """ë‘ ì¥ì†Œì˜ GPS ì¢Œí‘œê°€ ê°€ê¹Œìš´ì§€ í™•ì¸"""
    try:
        lat_diff = abs(float(new_place['GPS_ìœ„ë„']) - float(existing_place['GPS_ìœ„ë„']))
        lon_diff = abs(float(new_place['GPS_ê²½ë„']) - float(existing_place['GPS_ê²½ë„']))
        return lat_diff < threshold and lon_diff < threshold
    except (TypeError, ValueError, KeyError):
        return False # ì¢Œí‘œ ì •ë³´ ì—†ìœ¼ë©´ False

def is_duplicate_enhanced(new_place: Dict[str, Any], existing_places: List[Dict[str, Any]],
                          name_threshold: int = 85, core_name_threshold: int = 90,
                          address_threshold: int = 80, gps_threshold_similar: float = 0.0005, # ìœ ì‚¬ë„ìš© GPS ê¸°ì¤€ (ì•½ 50m)
                          gps_threshold_core: float = GPS_SIMILARITY_THRESHOLD ) -> bool: # í•µì‹¬ í‚¤ì›Œë“œìš© GPS ê¸°ì¤€ (ì•½ 100m)
    """ê°•í™”ëœ ì¤‘ë³µ ê²€ì‚¬: ìœ ì‚¬ë„ + í•µì‹¬ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€"""

    new_core = get_core_keyword(new_place['ì¥ì†Œëª…'])

    for existing_place in existing_places:
        # 1. ê¸°ë³¸ì ì¸ ìœ ì‚¬ë„ ê²€ì‚¬ (ì´ë¦„/ì£¼ì†Œ/GPS)
        if is_similar_place(new_place, existing_place, name_threshold, address_threshold, gps_threshold_similar):
            st.write(f"[ì¤‘ë³µì œê±°(ìœ ì‚¬ë„)] '{new_place['ì¥ì†Œëª…']}'ëŠ” '{existing_place['ì¥ì†Œëª…']}'ê³¼(ì™€) ìœ ì‚¬ â†’ ìŠ¤í‚µ")
            return True

        # 2. í•µì‹¬ í‚¤ì›Œë“œ í¬í•¨ ê²€ì‚¬ (GPS ê·¼ì ‘ ì‹œ)
        existing_core = get_core_keyword(existing_place['ì¥ì†Œëª…'])
        # GPSê°€ ì„¤ì •ëœ ì„ê³„ê°’(gps_threshold_core) ì´ë‚´ë¡œ ê°€ê¹Œìš¸ ë•Œë§Œ í•µì‹¬ í‚¤ì›Œë“œ ê²€ì‚¬ ìˆ˜í–‰
        if is_gps_close(new_place, existing_place, gps_threshold_core):
            # ìƒˆ ì¥ì†Œ ì´ë¦„ì— ê¸°ì¡´ í•µì‹¬ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ê±°ë‚˜, ê·¸ ë°˜ëŒ€ ê²½ìš° (ë” ì—„ê²©í•˜ê²Œ)
            # ê·¸ë¦¬ê³  ë‘ í•µì‹¬ í‚¤ì›Œë“œê°€ ì–´ëŠ ì •ë„ ìœ ì‚¬í•  ë•Œ (ì™„ì „íˆ ë‹¤ë¥¸ ê³µì› ì´ë¦„ ë°©ì§€)
            core_sim = fuzz.token_set_ratio(new_core, existing_core)
            if (new_core in existing_place['ì¥ì†Œëª…'] or existing_core in new_place['ì¥ì†Œëª…']) and core_sim > 70: # í•µì‹¬ í‚¤ì›Œë“œ ìœ ì‚¬ë„ 70ì  ì´ìƒ
                st.write(f"[ì¤‘ë³µì œê±°(í•µì‹¬í‚¤ì›Œë“œ)] '{new_place['ì¥ì†Œëª…']}'ëŠ” '{existing_place['ì¥ì†Œëª…']}'ì˜ í•µì‹¬('{existing_core}')ê³¼ ê´€ë ¨ ìˆê³  ê°€ê¹Œì›€ â†’ ìŠ¤í‚µ")
                return True

    return False # ëª¨ë“  ê²€ì‚¬ë¥¼ í†µê³¼í•˜ë©´ ì¤‘ë³µ ì•„ë‹˜

# (is_similar_place í•¨ìˆ˜ëŠ” ì´ì „ ì½”ë“œì™€ ë™ì¼í•˜ê²Œ ìœ ì§€, ì—¬ê¸°ì„œ í˜¸ì¶œë¨)
def is_similar_place(new_place: Dict[str, Any], existing_place: Dict[str, Any],
                     name_threshold: int, address_threshold: int, gps_threshold: float) -> bool:
    """ì´ë¦„(token_set_ratio), ì£¼ì†Œ(token_set_ratio), GPS ê·¼ì ‘ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ë‘ ì¥ì†Œê°€ ìœ ì‚¬í•œì§€ íŒë‹¨"""

    if not all(k in new_place and new_place[k] is not None for k in ['ì¥ì†Œëª…', 'GPS_ìœ„ë„', 'GPS_ê²½ë„']) or \
       not all(k in existing_place and existing_place[k] is not None for k in ['ì¥ì†Œëª…', 'GPS_ìœ„ë„', 'GPS_ê²½ë„']):
        return False

    name_similarity = fuzz.token_set_ratio(new_place['ì¥ì†Œëª…'], existing_place['ì¥ì†Œëª…'])
    addr_similarity = 0
    if new_place.get('ì£¼ì†Œ') and existing_place.get('ì£¼ì†Œ'):
        addr_similarity = fuzz.token_set_ratio(str(new_place['ì£¼ì†Œ']), str(existing_place['ì£¼ì†Œ']))
    gps_close = is_gps_close(new_place, existing_place, gps_threshold)

    # ì´ë¦„ ìœ ì‚¬ë„ê°€ ë†’ê³  (ì£¼ì†Œ ìœ ì‚¬ë„ê°€ ë†’ê±°ë‚˜ || GPSê°€ ë§¤ìš° ê°€ê¹ê±°ë‚˜)
    if name_similarity >= name_threshold and (addr_similarity >= address_threshold or gps_close):
        return True
    return False


# =========================================================
# 4. API ê¸°ë°˜ ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜ (ê°•í™”ëœ ì¤‘ë³µ ì œê±° ì ìš©)
# =========================================================
def crawl_api_data_with_images(keywords: List[str], max_api_pages: int) -> pd.DataFrame:
    all_data = [] # ìµœì¢… ì €ì¥ë , ì¤‘ë³µ ì œê±°ëœ ë°ì´í„°
    total_bar = st.progress(0, text="ì „ì²´ í‚¤ì›Œë“œ ì§„í–‰ë¥ ")

    for i, keyword in enumerate(keywords):
        st.info(f"--- '{keyword}' ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ (API) ---")
        page = 1
        is_end = False
        processed_api_ids_this_keyword = set() # API í˜ì´ì§• ì¤‘ë³µ ë°©ì§€

        while page <= max_api_pages and not is_end:
            # st.write(f"  > API í˜ì´ì§€ {page} ìˆ˜ì§‘ ì¤‘...") # ë¡œê·¸ ê°„ì†Œí™”
            json_data = search_kakao_local_api(keyword, page)
            if not json_data: break

            documents = json_data.get('documents', [])
            is_end = json_data.get('meta', {}).get('is_end', True)
            if not documents: break

            for doc_index, doc in enumerate(documents):
                place_id = doc.get('id')
                if place_id in processed_api_ids_this_keyword: continue
                processed_api_ids_this_keyword.add(place_id)

                place_name = doc.get('place_name')
                address = doc.get('road_address_name') or doc.get('address_name')
                category_name = doc.get('category_name')
                latitude = doc.get('y')
                longitude = doc.get('x')

                if not place_name or not address or not latitude or not longitude:
                    st.warning(f"  -> í•„ìˆ˜ ì •ë³´ ëˆ„ë½ ìŠ¤í‚µ: {place_name}")
                    continue

                new_entry = {
                    'ì¥ì†Œëª…': place_name.strip(),
                    'ì¹´í…Œê³ ë¦¬_RAW': category_name,
                    'ì£¼ì†Œ': address.strip(),
                    'GPS_ìœ„ë„': float(latitude),
                    'GPS_ê²½ë„': float(longitude),
                    'Kakao_Place_ID': place_id,
                    'í¬ë¡¤ë§_ì†ŒìŠ¤': 'KakaoAPI'
                }

                # âœ¨ ê°•í™”ëœ ì¤‘ë³µ ì²´í¬ í˜¸ì¶œ âœ¨
                if is_duplicate_enhanced(new_entry, all_data):
                    continue # ì¤‘ë³µì´ë©´ ë‹¤ìŒ ì¥ì†Œë¡œ

                # ğŸ” ë‹¤ì¤‘ ì´ë¯¸ì§€ ê²€ìƒ‰ (ì¤‘ë³µì´ ì•„ë‹ ê²½ìš°ì—ë§Œ ìˆ˜í–‰)
                try:
                    # ì´ë¯¸ì§€ ê²€ìƒ‰ ì‹œ 'ì§€ì—­' ì •ë³´ ì¶”ê°€í•˜ì—¬ ì •í™•ë„ í–¥ìƒ
                    region_hint = keyword.split()[0] if len(keyword.split()) > 1 else ""
                    search_query = f"{region_hint} {place_name}".strip()
                    image_urls = search_kakao_image_api_multi(search_query, count=MAX_IMAGES_PER_PLACE)
                    new_entry['image_urls'] = ", ".join(image_urls)
                except Exception as e:
                    st.warning(f"  -> {place_name} ì´ë¯¸ì§€ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                    new_entry['image_urls'] = ""

                all_data.append(new_entry) # ìµœì¢… ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€

            page += 1
            time.sleep(0.3)

        total_bar.progress((i + 1) / len(keywords), text=f"í‚¤ì›Œë“œ: {keyword} ìˆ˜ì§‘ ì™„ë£Œ.")

    st.success(f"âœ… ì´ {len(all_data)}ê°œ ì¥ì†Œ ìˆ˜ì§‘ ì™„ë£Œ (ê°•í™”ëœ ì¤‘ë³µ ì œê±° ì ìš©)")
    return pd.DataFrame(all_data)


# =========================================================
# 5. AI í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ (ZIP ì••ì¶•)
# =========================================================
def download_images_for_ai_test(df, total_goal):
    if 'image_urls' not in df.columns: return None, 0

    all_urls = []
    for urls_str in df['image_urls'].dropna().astype(str):
        urls_in_row = [url.strip() for url in urls_str.split(',') if url.strip().startswith('http')]
        all_urls.extend(urls_in_row)

    unique_urls = list(dict.fromkeys(all_urls))
    if not unique_urls: return None, 0

    urls_to_download = unique_urls[:total_goal]

    zip_buffer = io.BytesIO()
    download_count = 0
    headers = {
        'Referer': 'https://developers.kakao.com/',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36'
    }

    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zf:
        progress_bar = st.progress(0, text=f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì¤‘ (0/{len(urls_to_download)})")
        for i, url in enumerate(urls_to_download):
            try:
                response = requests.get(url, headers=headers, timeout=15)
                response.raise_for_status()

                file_name = f"image_{i+1}.jpg" # ê¸°ë³¸ íŒŒì¼ëª…
                try:
                    mask = df['image_urls'].astype(str).str.contains(re.escape(url), na=False)
                    if mask.any():
                        row = df[mask].iloc[0]
                        category_part = str(row.get('ì¹´í…Œê³ ë¦¬_RAW', 'Unknown')).split('>')[-1].strip()
                        place_part = str(row.get('ì¥ì†Œëª…', f'Place{i+1}'))
                        category = re.sub(r'[\\/*?:"<>|]', '', category_part)
                        place_name = re.sub(r'[\\/*?:"<>|]', '', place_part)
                        file_name = f"{category[:10]}_{place_name[:20]}_{i+1}.jpg"
                except Exception: pass

                zf.writestr(f"ai_test_images/{file_name}", response.content)
                download_count += 1
            except Exception as e:
                st.warning(f"  -> ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {url[:50]} ({e})")

            progress_bar.progress((i + 1) / len(urls_to_download), text=f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì¤‘ ({i+1}/{len(urls_to_download)})")

    progress_bar.empty()
    return zip_buffer, download_count


# =========================================================
# 6. Streamlit UI
# =========================================================
def app():
    st.set_page_config(page_title="ì¹´ì¹´ì˜¤ API í¬ë¡¤ëŸ¬", layout="wide")
    st.title("ğŸ—ºï¸ ë¼ì´í”„ ë ˆì½”ë”: ì¹´ì¹´ì˜¤ API ë°ì´í„° í¬ë¡¤ëŸ¬")
    st.markdown("##### **API ê¸°ë°˜ ìˆ˜ì§‘** + **ê°•í™”ëœ ìœ ì‚¬ë„/í•µì‹¬í‚¤ì›Œë“œ ê¸°ë°˜ ì¤‘ë³µ ì œê±°** ì ìš©")
    st.success("âœ… ì´ë¦„/ìœ„ì¹˜ ìœ ì‚¬ ì‹œ í•µì‹¬ í‚¤ì›Œë“œê°€ ê°™ìœ¼ë©´ í•˜ë‚˜ë¡œ ì²˜ë¦¬ (ì˜ˆ: ê³µì› ë‚´ ë‹¤ë¥¸ ì‹œì„¤ ì¤‘ë³µ ë°©ì§€).")
    st.divider()

    if 'api_image_data' not in st.session_state:
        st.session_state.api_image_data = pd.DataFrame()

    # 1ï¸âƒ£ ë°ì´í„° ìˆ˜ì§‘ ì„¤ì •
    st.header("1. ë°ì´í„° ìˆ˜ì§‘ ì„¤ì •")
    col1, col2 = st.columns(2)
    with col1: region_input = st.text_area("ê²€ìƒ‰ ì§€ì—­", '\n'.join(TARGET_REGIONS), height=100)
    with col2: theme_input = st.text_area("ê²€ìƒ‰ í…Œë§ˆ", '\n'.join(TARGET_THEMES), height=150)
    pages_input = st.number_input("í‚¤ì›Œë“œë‹¹ ìµœëŒ€ API í˜ì´ì§€", 1, 15, MAX_API_PAGES)

    # 2ï¸âƒ£ ìˆ˜ì§‘ ì‹¤í–‰
    if st.button("ğŸš€ API ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ (ì¤‘ë³µ ì œê±° ê°•í™”)"):
        st.session_state.api_image_data = pd.DataFrame()
        regions = [r.strip() for r in region_input.split('\n') if r.strip()]
        themes = [t.strip() for t in theme_input.split('\n') if t.strip()]
        keywords = [f"{r} {t}" for r in regions for t in themes]

        if not keywords: st.error("ì§€ì—­ê³¼ í…Œë§ˆë¥¼ ì…ë ¥í•˜ì„¸ìš”."); return

        st.info(f"ì´ {len(keywords)}ê°œ í‚¤ì›Œë“œë¡œ API ìˆ˜ì§‘ ì‹œì‘...")
        with st.spinner("API í˜¸ì¶œ ë° ê°•í™”ëœ ì¤‘ë³µ ì œê±° ì§„í–‰ ì¤‘..."):
            final_df = crawl_api_data_with_images(keywords, pages_input)

        if final_df.empty:
            st.error("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.session_state.api_image_data = pd.DataFrame()
            st.subheader("ìˆ˜ì§‘ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
            st.dataframe(st.session_state.api_image_data)
            return

        # --- ìµœì¢… ë°ì´í„° ì²˜ë¦¬ ë° ì €ì¥ ---
        original_count = len(final_df) # ê°•í™”ëœ ìœ ì‚¬ë„ í•„í„° í›„ count

        # ìµœì¢… ID ì¤‘ë³µ ì œê±° (ì•ˆì „ ì¥ì¹˜)
        final_df['Kakao_Place_ID'] = final_df['Kakao_Place_ID'].astype(str).fillna('UNKNOWN_ID')
        final_df.drop_duplicates(subset=['Kakao_Place_ID'], keep='first', inplace=True)
        final_df = final_df[final_df['Kakao_Place_ID'] != 'UNKNOWN_ID'].reset_index(drop=True)
        final_dedup_count = len(final_df)

        final_df = final_df.astype(object).where(pd.notnull(final_df), None)
        st.session_state.api_image_data = final_df

        csv_filename = "kakao_api_places_final_dedup.csv" # íŒŒì¼ëª… ë³€ê²½
        try:
            final_df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
            st.success(f"âœ… ìˆ˜ì§‘ ì„±ê³µ! ìœ ì‚¬ë„/í•µì‹¬í‚¤ì›Œë“œ í•„í„° í›„ {original_count}ê±´, ìµœì¢… ID ì¤‘ë³µ ì œê±° í›„ {final_dedup_count}ê±´ í™•ë³´.")
            st.info(f"ğŸ’¾ ë°ì´í„°ê°€ `{csv_filename}` íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e_csv: st.error(f"CSV ì €ì¥ ì‹¤íŒ¨: {e_csv}")

        st.subheader(f"ìˆ˜ì§‘ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ìµœì¢… {final_dedup_count}ê±´)")
        display_df = final_df.copy()
        if 'image_urls' in display_df.columns:
            display_df['image_urls_display'] = display_df['image_urls'].apply(
                lambda x: (str(x).split(',')[0].strip()[:40] + '...') if pd.notna(x) and str(x) else None
            )
            st.dataframe(display_df.drop(columns=['image_urls']))
        else:
            st.dataframe(display_df)

    st.divider()

    # 3ï¸âƒ£ AI í…ŒìŠ¤íŠ¸ìš© ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
    st.header("3. AI ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ")
    if not st.session_state.api_image_data.empty:
        df_display = st.session_state.api_image_data
        total_unique_urls = 0
        if 'image_urls' in df_display.columns:
             all_urls_for_count = []
             for urls_str in df_display['image_urls'].dropna().astype(str):
                  urls_in_row = [url.strip() for url in urls_str.split(',') if url.strip().startswith('http')]
                  all_urls_for_count.extend(urls_in_row)
             total_unique_urls = len(list(dict.fromkeys(all_urls_for_count)))

        st.info(f"ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥í•œ ì´ë¯¸ì§€ URL: **{total_unique_urls}ê°œ**, ëª©í‘œ: {AI_TEST_IMAGE_COUNT}ì¥")
        zip_buffer, downloaded_count = download_images_for_ai_test(df_display, AI_TEST_IMAGE_COUNT)

        if zip_buffer and downloaded_count > 0:
            st.download_button(
                label=f"âœ… ai_test_images.zip ë‹¤ìš´ë¡œë“œ ({downloaded_count}ì¥ ì••ì¶•ë¨)",
                data=zip_buffer.getvalue(), file_name="ai_test_images.zip", mime="application/zip", key="download_button_final"
            )
        elif downloaded_count == 0 and total_unique_urls > 0: st.error("ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨. ë¡œê·¸ í™•ì¸.")
        elif total_unique_urls == 0 : st.warning("í¬ë¡¤ë§ëœ ë°ì´í„°ì— ìœ íš¨í•œ ì´ë¯¸ì§€ URLì´ ì—†ìŠµë‹ˆë‹¤.")
        else: st.warning("ë‹¤ìš´ë¡œë“œ ì¤€ë¹„ ì¤‘ ì˜¤ë¥˜.")
    else:
        st.warning("ë¨¼ì € API ë°ì´í„° ìˆ˜ì§‘ì„ ì™„ë£Œí•´ì£¼ì„¸ìš”.")

if __name__ == '__main__':
    app()

